Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32
Type "copyright", "credits" or "license()" for more information.
>>> 
================ RESTART: C:\Users\Soura\Desktop\my_train.py ================

Warning (from warnings module):
  File "C:\Users\Soura\AppData\Local\Programs\Python\Python36\lib\site-packages\gensim\utils.py", line 1167
    warnings.warn("detected Windows; aliasing chunkize to chunkize_serial")
UserWarning: detected Windows; aliasing chunkize to chunkize_serial

Warning (from warnings module):
  File "C:\Users\Soura\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\cross_validation.py", line 41
    "This module will be removed in 0.20.", DeprecationWarning)
DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
dataset loaded with shape (9294, 2)
progress-bar:   0%|          | 0/9294 [00:00<?, ?it/s]progress-bar:  26%|##6       | 2452/9294 [00:00<00:00, 24418.78it/s]progress-bar:  53%|#####2    | 4897/9294 [00:00<00:00, 24408.37it/s]progress-bar:  73%|#######3  | 6791/9294 [00:00<00:00, 22393.95it/s]progress-bar:  95%|#########5| 8835/9294 [00:00<00:00, 21740.31it/s]progress-bar: 100%|##########| 9294/9294 [00:00<00:00, 21520.77it/s]
0it [00:00, ?it/s]7435it [00:00, 73771.76it/s]
0it [00:00, ?it/s]1859it [00:00, 264877.91it/s]
  0%|          | 0/7435 [00:00<?, ?it/s]100%|##########| 7435/7435 [00:00<00:00, 1994286.00it/s]
  0%|          | 0/7435 [00:00<?, ?it/s]100%|##########| 7435/7435 [00:00<00:00, 1849294.33it/s]
vocab size : 1310
0it [00:00, ?it/s]792it [00:00, 7896.92it/s]1646it [00:00, 8207.05it/s]2507it [00:00, 8332.98it/s]3310it [00:00, 8250.23it/s]4126it [00:00, 8227.85it/s]4901it [00:00, 8134.99it/s]5685it [00:00, 8087.23it/s]6442it [00:00, 8019.59it/s]7237it [00:00, 8011.06it/s]7435it [00:00, 7907.35it/s]
0it [00:00, ?it/s]859it [00:00, 8566.93it/s]1728it [00:00, 8616.93it/s]1859it [00:00, 8503.52it/s]
Using TensorFlow backend.
Epoch 1/200
 - 6s - loss: 0.6935 - acc: 0.5087
Epoch 2/200
 - 6s - loss: 0.6822 - acc: 0.7095
Epoch 3/200
 - 6s - loss: 0.6185 - acc: 0.7856
Epoch 4/200
 - 6s - loss: 0.5360 - acc: 0.7900
Epoch 5/200
 - 5s - loss: 0.5045 - acc: 0.7942
Epoch 6/200
 - 6s - loss: 0.4971 - acc: 0.7948
Epoch 7/200
 - 6s - loss: 0.4928 - acc: 0.7976
Epoch 8/200
 - 6s - loss: 0.4858 - acc: 0.8005
Epoch 9/200
 - 6s - loss: 0.4739 - acc: 0.8071
Epoch 10/200
 - 6s - loss: 0.4757 - acc: 0.8058
Epoch 11/200
 - 6s - loss: 0.4786 - acc: 0.8065
Epoch 12/200
 - 6s - loss: 0.4754 - acc: 0.8065
Epoch 13/200
 - 6s - loss: 0.4727 - acc: 0.8069
Epoch 14/200
 - 6s - loss: 0.4721 - acc: 0.8090
Epoch 15/200
 - 6s - loss: 0.4772 - acc: 0.8027
Epoch 16/200
 - 6s - loss: 0.4711 - acc: 0.8077
Epoch 17/200
 - 6s - loss: 0.4741 - acc: 0.8051
Epoch 18/200
 - 6s - loss: 0.4666 - acc: 0.8118
Epoch 19/200
 - 6s - loss: 0.4691 - acc: 0.8073
Epoch 20/200
 - 6s - loss: 0.4602 - acc: 0.8109
Epoch 21/200
 - 6s - loss: 0.4593 - acc: 0.8106
Epoch 22/200
 - 6s - loss: 0.4545 - acc: 0.8157
Epoch 23/200
 - 6s - loss: 0.4520 - acc: 0.8155
Epoch 24/200
 - 6s - loss: 0.4498 - acc: 0.8157
Epoch 25/200
 - 6s - loss: 0.4506 - acc: 0.8148
Epoch 26/200
 - 6s - loss: 0.4442 - acc: 0.8182
Epoch 27/200
 - 6s - loss: 0.4463 - acc: 0.8192
Epoch 28/200
 - 6s - loss: 0.4468 - acc: 0.8202
Epoch 29/200
 - 6s - loss: 0.4454 - acc: 0.8207
Epoch 30/200
 - 6s - loss: 0.4449 - acc: 0.8199
Epoch 31/200
 - 6s - loss: 0.4422 - acc: 0.8229
Epoch 32/200
 - 6s - loss: 0.4388 - acc: 0.8264
Epoch 33/200
 - 6s - loss: 0.4401 - acc: 0.8247
Epoch 34/200
 - 6s - loss: 0.4412 - acc: 0.8250
Epoch 35/200
 - 6s - loss: 0.4398 - acc: 0.8218
Epoch 36/200
 - 6s - loss: 0.4401 - acc: 0.8246
Epoch 37/200
 - 6s - loss: 0.4382 - acc: 0.8235
Epoch 38/200
 - 6s - loss: 0.4367 - acc: 0.8247
Epoch 39/200
 - 6s - loss: 0.4397 - acc: 0.8256
Epoch 40/200
 - 6s - loss: 0.4379 - acc: 0.8266
Epoch 41/200
 - 6s - loss: 0.4361 - acc: 0.8268
Epoch 42/200
 - 6s - loss: 0.4337 - acc: 0.8286
Epoch 43/200
 - 6s - loss: 0.4378 - acc: 0.8277
Epoch 44/200
 - 6s - loss: 0.4357 - acc: 0.8285
Epoch 45/200
 - 6s - loss: 0.4351 - acc: 0.8278
Epoch 46/200
 - 6s - loss: 0.4348 - acc: 0.8295
Epoch 47/200
 - 6s - loss: 0.4350 - acc: 0.8300
Epoch 48/200
 - 6s - loss: 0.4335 - acc: 0.8284
Epoch 49/200
 - 6s - loss: 0.4331 - acc: 0.8316
Epoch 50/200
 - 6s - loss: 0.4323 - acc: 0.8296
Epoch 51/200
 - 6s - loss: 0.4328 - acc: 0.8311
Epoch 52/200
 - 6s - loss: 0.4327 - acc: 0.8301
Epoch 53/200
 - 6s - loss: 0.4323 - acc: 0.8309
Epoch 54/200
 - 6s - loss: 0.4322 - acc: 0.8319
Epoch 55/200
 - 7s - loss: 0.4298 - acc: 0.8324
Epoch 56/200
 - 6s - loss: 0.4300 - acc: 0.8301
Epoch 57/200
 - 6s - loss: 0.4321 - acc: 0.8323
Epoch 58/200
 - 6s - loss: 0.4301 - acc: 0.8316
Epoch 59/200
 - 6s - loss: 0.4281 - acc: 0.8320
Epoch 60/200
 - 6s - loss: 0.4302 - acc: 0.8320
Epoch 61/200
 - 6s - loss: 0.4278 - acc: 0.8338
Epoch 62/200
 - 6s - loss: 0.4307 - acc: 0.8321
Epoch 63/200
 - 6s - loss: 0.4299 - acc: 0.8320
Epoch 64/200
 - 6s - loss: 0.4298 - acc: 0.8315
Epoch 65/200
 - 6s - loss: 0.4282 - acc: 0.8305
Epoch 66/200
 - 5s - loss: 0.4264 - acc: 0.8332
Epoch 67/200
 - 6s - loss: 0.4286 - acc: 0.8303
Epoch 68/200
 - 6s - loss: 0.4274 - acc: 0.8350
Epoch 69/200
 - 6s - loss: 0.4303 - acc: 0.8348
Epoch 70/200
 - 6s - loss: 0.4288 - acc: 0.8343
Epoch 71/200
 - 6s - loss: 0.4288 - acc: 0.8360
Epoch 72/200
 - 6s - loss: 0.4288 - acc: 0.8342
Epoch 73/200
 - 6s - loss: 0.4326 - acc: 0.8331
Epoch 74/200
 - 6s - loss: 0.4284 - acc: 0.8320
Epoch 75/200
 - 6s - loss: 0.4266 - acc: 0.8301
Epoch 76/200
 - 6s - loss: 0.4302 - acc: 0.8331
Epoch 77/200
 - 6s - loss: 0.4291 - acc: 0.8332
Epoch 78/200
 - 6s - loss: 0.4256 - acc: 0.8319
Epoch 79/200
 - 7s - loss: 0.4286 - acc: 0.8332
Epoch 80/200
 - 6s - loss: 0.4258 - acc: 0.8336
Epoch 81/200
 - 6s - loss: 0.4282 - acc: 0.8339
Epoch 82/200
 - 6s - loss: 0.4258 - acc: 0.8348
Epoch 83/200
 - 6s - loss: 0.4281 - acc: 0.8347
Epoch 84/200
 - 6s - loss: 0.4257 - acc: 0.8373
Epoch 85/200
 - 6s - loss: 0.4278 - acc: 0.8358
Epoch 86/200
 - 6s - loss: 0.4265 - acc: 0.8328
Epoch 87/200
 - 6s - loss: 0.4245 - acc: 0.8369
Epoch 88/200
 - 6s - loss: 0.4284 - acc: 0.8344
Epoch 89/200
 - 5s - loss: 0.4278 - acc: 0.8371
Epoch 90/200
 - 6s - loss: 0.4277 - acc: 0.8381
Epoch 91/200
 - 7s - loss: 0.4259 - acc: 0.8346
Epoch 92/200
 - 6s - loss: 0.4284 - acc: 0.8363
Epoch 93/200
 - 6s - loss: 0.4273 - acc: 0.8375
Epoch 94/200
 - 6s - loss: 0.4244 - acc: 0.8381
Epoch 95/200
 - 6s - loss: 0.4268 - acc: 0.8356
Epoch 96/200
 - 6s - loss: 0.4246 - acc: 0.8383
Epoch 97/200
 - 6s - loss: 0.4292 - acc: 0.8352
Epoch 98/200
 - 6s - loss: 0.4297 - acc: 0.8352
Epoch 99/200
 - 6s - loss: 0.4255 - acc: 0.8381
Epoch 100/200
 - 6s - loss: 0.4267 - acc: 0.8362
Epoch 101/200
 - 6s - loss: 0.4273 - acc: 0.8373
Epoch 102/200
 - 6s - loss: 0.4251 - acc: 0.8395
Epoch 103/200
 - 6s - loss: 0.4265 - acc: 0.8393
Epoch 104/200
 - 6s - loss: 0.4253 - acc: 0.8398
Epoch 105/200
 - 6s - loss: 0.4254 - acc: 0.8394
Epoch 106/200
 - 6s - loss: 0.4263 - acc: 0.8378
Epoch 107/200
 - 6s - loss: 0.4265 - acc: 0.8398
Epoch 108/200
 - 6s - loss: 0.4243 - acc: 0.8385
Epoch 109/200
 - 6s - loss: 0.4251 - acc: 0.8385
Epoch 110/200
 - 6s - loss: 0.4237 - acc: 0.8391
Epoch 111/200
 - 6s - loss: 0.4304 - acc: 0.8413
Epoch 112/200
 - 6s - loss: 0.4284 - acc: 0.8391
Epoch 113/200
 - 7s - loss: 0.4262 - acc: 0.8378
Epoch 114/200
 - 6s - loss: 0.4257 - acc: 0.8366
Epoch 115/200
 - 7s - loss: 0.4249 - acc: 0.8387
Epoch 116/200
 - 6s - loss: 0.4257 - acc: 0.8373
Epoch 117/200
 - 6s - loss: 0.4256 - acc: 0.8426
Epoch 118/200
 - 5s - loss: 0.4266 - acc: 0.8398
Epoch 119/200
 - 6s - loss: 0.4248 - acc: 0.8403
Epoch 120/200
 - 5s - loss: 0.4250 - acc: 0.8399
Epoch 121/200
 - 6s - loss: 0.4268 - acc: 0.8405
Epoch 122/200
 - 6s - loss: 0.4230 - acc: 0.8428
Epoch 123/200
 - 6s - loss: 0.4249 - acc: 0.8394
Epoch 124/200
 - 6s - loss: 0.4276 - acc: 0.8393
Epoch 125/200
 - 6s - loss: 0.4240 - acc: 0.8424
Epoch 126/200
 - 6s - loss: 0.4240 - acc: 0.8428
Epoch 127/200
 - 6s - loss: 0.4213 - acc: 0.8421
Epoch 128/200
 - 6s - loss: 0.4262 - acc: 0.8421
Epoch 129/200
 - 6s - loss: 0.4271 - acc: 0.8408
Epoch 130/200
 - 6s - loss: 0.4235 - acc: 0.8386
Epoch 131/200
 - 6s - loss: 0.4251 - acc: 0.8408
Epoch 132/200
 - 6s - loss: 0.4224 - acc: 0.8391
Epoch 133/200
 - 6s - loss: 0.4262 - acc: 0.8412
Epoch 134/200
 - 6s - loss: 0.4247 - acc: 0.8422
Epoch 135/200
 - 6s - loss: 0.4242 - acc: 0.8413
Epoch 136/200
 - 6s - loss: 0.4243 - acc: 0.8434
Epoch 137/200
 - 6s - loss: 0.4263 - acc: 0.8418
Epoch 138/200
 - 6s - loss: 0.4268 - acc: 0.8410
Epoch 139/200
 - 6s - loss: 0.4251 - acc: 0.8426
Epoch 140/200
 - 6s - loss: 0.4272 - acc: 0.8408
Epoch 141/200
 - 6s - loss: 0.4222 - acc: 0.8418
Epoch 142/200
 - 6s - loss: 0.4267 - acc: 0.8408
Epoch 143/200
 - 6s - loss: 0.4257 - acc: 0.8414
Epoch 144/200
 - 6s - loss: 0.4243 - acc: 0.8422
Epoch 145/200
 - 6s - loss: 0.4259 - acc: 0.8426
Epoch 146/200
 - 6s - loss: 0.4287 - acc: 0.8425
Epoch 147/200
 - 6s - loss: 0.4283 - acc: 0.8403
Epoch 148/200
 - 6s - loss: 0.4253 - acc: 0.8452
Epoch 149/200
 - 6s - loss: 0.4252 - acc: 0.8432
Epoch 150/200
 - 6s - loss: 0.4243 - acc: 0.8414
Epoch 151/200
 - 6s - loss: 0.4245 - acc: 0.8433
Epoch 152/200
 - 6s - loss: 0.4244 - acc: 0.8443
Epoch 153/200
 - 6s - loss: 0.4275 - acc: 0.8418
Epoch 154/200
 - 6s - loss: 0.4241 - acc: 0.8441
Epoch 155/200
 - 6s - loss: 0.4260 - acc: 0.8425
Epoch 156/200
 - 6s - loss: 0.4247 - acc: 0.8433
Epoch 157/200
 - 6s - loss: 0.4252 - acc: 0.8416
Epoch 158/200
 - 6s - loss: 0.4254 - acc: 0.8429
Epoch 159/200
 - 6s - loss: 0.4256 - acc: 0.8412
Epoch 160/200
 - 6s - loss: 0.4236 - acc: 0.8444
Epoch 161/200
 - 6s - loss: 0.4228 - acc: 0.8403
Epoch 162/200
 - 6s - loss: 0.4243 - acc: 0.8420
Epoch 163/200
 - 6s - loss: 0.4225 - acc: 0.8422
Epoch 164/200
 - 6s - loss: 0.4246 - acc: 0.8416
Epoch 165/200
 - 6s - loss: 0.4227 - acc: 0.8444
Epoch 166/200
 - 6s - loss: 0.4250 - acc: 0.8418
Epoch 167/200
 - 6s - loss: 0.4240 - acc: 0.8432
Epoch 168/200
 - 6s - loss: 0.4268 - acc: 0.8440
Epoch 169/200
 - 6s - loss: 0.4273 - acc: 0.8440
Epoch 170/200
 - 6s - loss: 0.4264 - acc: 0.8441
Epoch 171/200
 - 6s - loss: 0.4249 - acc: 0.8409
Epoch 172/200
 - 6s - loss: 0.4228 - acc: 0.8448
Epoch 173/200
 - 6s - loss: 0.4240 - acc: 0.8451
Epoch 174/200
 - 6s - loss: 0.4239 - acc: 0.8405
Epoch 175/200
 - 6s - loss: 0.4264 - acc: 0.8403
Epoch 176/200
 - 6s - loss: 0.4223 - acc: 0.8455
Epoch 177/200
 - 6s - loss: 0.4262 - acc: 0.8437
Epoch 178/200
 - 6s - loss: 0.4237 - acc: 0.8452
Epoch 179/200
 - 6s - loss: 0.4234 - acc: 0.8426
Epoch 180/200
 - 6s - loss: 0.4238 - acc: 0.8432
Epoch 181/200
 - 6s - loss: 0.4256 - acc: 0.8443
Epoch 182/200
 - 6s - loss: 0.4282 - acc: 0.8430
Epoch 183/200
 - 6s - loss: 0.4291 - acc: 0.8449
Epoch 184/200
 - 6s - loss: 0.4237 - acc: 0.8441
Epoch 185/200
 - 6s - loss: 0.4271 - acc: 0.8448
Epoch 186/200
 - 6s - loss: 0.4273 - acc: 0.8445
Epoch 187/200
 - 6s - loss: 0.4245 - acc: 0.8444
Epoch 188/200
 - 6s - loss: 0.4282 - acc: 0.8447
Epoch 189/200
 - 6s - loss: 0.4230 - acc: 0.8451
Epoch 190/200
 - 6s - loss: 0.4252 - acc: 0.8433
Epoch 191/200
 - 6s - loss: 0.4274 - acc: 0.8445
Epoch 192/200
 - 6s - loss: 0.4270 - acc: 0.8434
Epoch 193/200
 - 6s - loss: 0.4280 - acc: 0.8449
Epoch 194/200
 - 6s - loss: 0.4255 - acc: 0.8437
Epoch 195/200
 - 6s - loss: 0.4256 - acc: 0.8440
Epoch 196/200
 - 6s - loss: 0.4274 - acc: 0.8428
Epoch 197/200
 - 6s - loss: 0.4229 - acc: 0.8430
Epoch 198/200
 - 6s - loss: 0.4268 - acc: 0.8425
Epoch 199/200
 - 6s - loss: 0.4252 - acc: 0.8449
Epoch 200/200
 - 6s - loss: 0.4275 - acc: 0.8448
Traceback (most recent call last):
  File "C:\Users\Soura\Desktop\my_train.py", line 126, in <module>
    model.add(LSTM(128,batch_input_shape=(batch_size,timesteps,data_dim)))
  File "C:\Users\Soura\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\models.py", line 492, in add
    output_tensor = layer(self.outputs[0])
  File "C:\Users\Soura\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\layers\recurrent.py", line 488, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File "C:\Users\Soura\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\topology.py", line 573, in __call__
    self.assert_input_compatibility(inputs)
  File "C:\Users\Soura\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\topology.py", line 472, in assert_input_compatibility
    str(K.ndim(x)))
ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2
>>> 
